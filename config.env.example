# Ollama Configuration
OLLAMA_API_BASE=http://localhost:11434
OLLAMA_MODEL=qwen3:latest

# Optional: Adjust context window size (default: 8192)
OLLAMA_CONTEXT_SIZE=8192

# Optional: Adjust temperature for response creativity (0.0-1.0)
OLLAMA_TEMPERATURE=0.1

# Optional: Maximum iterations for agent reasoning
MAX_ITERATIONS=10 